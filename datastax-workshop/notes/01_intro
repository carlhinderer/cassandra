-----------------------------------------------------------------------------
| CHAPTER 1 - INTRODUCTION                                                  |
-----------------------------------------------------------------------------

- DataStax Course

    - Notes here from DS 201 playlist:

        https://www.youtube.com/
        watch?v=69pvhO6mK_o&list=PL2g2h-wyI4Spf5rzSmesewHpXYVnyQ2TS&ab_channel=DataStaxDevelopers


    - Repo for Workshop:

        https://github.com/datastaxdevs/workshop-intro-to-cassandra



- Cassandra Primary Keys

    - To define a PK:

        PRIMARY KEY ((partitioning), cluster)


    - The partitioning columns determine where data is stored.
      The clustering columns determine how data is sorted.


    - Once you've created a table, you can't change the PK with an ALTER TABLE.  You'll need to recreate
        the table to change the key.


    - This key isn't unique enough, so records will be overwritten with upserts:

        PRIMARY KEY ((state), city, name)

      We'll use this instead to provide enough uniqueness to identify a single row:

        PRIMARY KEY ((state), city, name, id)


    - The PK actually solves 3 problems in Cassandra:

        1. How to partition data
        2. How to sort data in partitions
        3. Uniqueness


    - We need to put columns in the correct order (the way they were defined in the primary key) in
        queries.


    - Sometimes, we may want clustering columns to store data in DESC order rather that ASC.  Time series
        data is a common use for this.

        PRIMARY KEY ((state), city, name, id)
        WITH CLUSTERING ORDER BY (city DESC, name ASC)


    - Tables are named with the partition key by convention.

        users_by_city
        posts_by_user
        posts_by_room


    - We usually want one table per query.



- Immediate Consistency

    - We get immediate consistency (read is always last write) if 

        CL(read) + CL(write) > RF


    - Example

        RF = 3
        CL(write) = ALL (3)
        CL(read)  = 1

        4 > 3


    - Example

        RF = 3
        CL(write) = QUORUM (2)
        CL(read)  = QUORUM (2)

        4 > 3



- Nodes and VNodes

    - We want to use an attached hard drive, not a SAN for our nodes.  If the disk is connected via
        an ethernet cable, it's the wrong solution.


    - Statues for Node = [Joining, Leaving, Up, Down]


    - The 'bin/nodetool' is used to manage individual nodes.


    - One node maps to multiple vnodes.  This makes moving data less painful when nodes are added and
        removed.



- Gossip and Snitches

    - The Gossip protocol is used to share cluster metadata.  Each node picks 1-3 nodes every second
        to gossip with.


    - Snitches are used to determine and declare each node's rack and data center.  There are 4 types:

        1. SimpleSnitch = Put all nodes in the same data center and rack

        2. PropertyFileSnitch = Read datacenter and rack information for all nodes from a file
                                  (cassandra-topology.properties), it is a pain to maintain this file
                                  on all nodes

        3. GossipingPropertyFileSnitch = Just maintain each individual node's configuration, and let
                                           gossip spread the information

        4. RackInferringSnitch = Try to guess data center and rack from IP address (not recommended)



- Replication

    - Murmum3 and MD5 are consistent hashing algorithms used for partitioning.


    - To use multi-datacenter replication:

        CREATE KEYSPACE killrvideo
        WITH REPLICATION = {
            'class': 'NetworkTopologyStrategy',
            'dc-west': 2,
            'dc-east': 3
        }



- Consistency Levels

    - Strong Consistency

        CL(write) = QUORUM
        CL(read)  = QUORUM

        Most recent write is always read.


    - Eventual Consistency

        CL(write) = 1
        CL(read)  = 1

        We're still confirming the write/read was successful, but it will take a short amount of time to
          propogate to all replicas (~ 1 ms).  This is often fine for things like log messages, IOT
          readings, and time series.


    - Multi-Datacenter Consistency

        - Use a quorum only in the local center.  Nodes in other datacenters are not considered for 
            quorums.

        - This helps avoid cross-datacenter waiting ( ~ 70 ms from us-west to us-east).


    - Here are all the consistency levels available in Cassandra:

        ANY                    # No replication, just throw data into cluster (don't use)

        ONE, TWO, THREE        # Checks closest nodes to coordinator

        QUORUM                 # Majority vote

        LOCAL_ONE              # Closest node in same datacenter

        LOCAL_QUORUM           # Closest quorum in same datacenter

        EACH_QUORUM            # Quorum of nodes in each datacenter

        ALL                    # Every node


        ONE and QUORUM are the most commonly used by far.



- Hinted Handoff

    - A node goes down, so a coordinator saves writes meant for the node in a file, and will send it to
        the node when it comes back up.

    - The coordinator keeps hints for 3 hours by default, and this can be configured.



- Read Repair

    - Normally, when reading, we just retrieve data from the first node, then checksums for the other
        nodes.  This is an optimization.

    - If the checksums don't match, read repair is launched asynchronously.



- Write Path

            ->   RAM                 ->  HD #2 
                 (Current SSTable)       (Older SSTables)
    WRITE
            ->   HD #1
                 (Commit Log)



- Read Path

    READ   ->   RAM          ->  SSTable #1  ->  SSTable #2  ->  SSTable #3  ->  ...
                (MemTable)


    - We also have:

        1. Partition Index (byte offsets for where each partition begins)

        2. Partition Index Summary (summarized version of partition index stored in RAM)

        3. Key Cache (if enabled, stores results of most recent queries)

        4. Bloom Filter (used to efficiently determine set membership)


    - Since data is always sorted by clustering columns, we can just use binary search once we know
        where the partition is.


    - If we use all the available read optimizations, the full read process looks like:

        READ  ->  Bloom Filter  ->  Key Cache  ->  Partition Summary  ->  Partition Index  ->  SSTable



- Compaction

    - Cassandra stores timestamps for every cell in every row, so we always know which value is most
        recent.


    - To delete a record, we write a tombstone.  We might leave tombstones around (~ 10 days) so that
        read repair doesn't make deleted data reappear.



- Compacting Partitions

    P1                                 P2
    --------------------               --------------------
    1   Johnny (85)                    1   Johnny (181)
    2   Betsy (49)                     2   X (186)
    3   Nichole (85)                   3   Norman (148)
    4   Sue (41)                       5   (X) (184)
    5   Sam (96)                       6   Henrie (134)

                            |
                            |
                            V

                    Compacted
                    --------------------
                    1   Johnny (181)
                    3   Norman (148)
                    4   Sue (141)
                    5   (X) (184)
                    6   Henrie (134)



- Compacting SSTables

    SSTable #1                          SSTable #2
    ------------------------            ------------------------
    7  13  18  21  58                   3  7  18  36  58  84

                                  |
                                  |
                                  V

                        Compacted
                        --------------------------------
                        3  7  13  18  21  36  58  84